{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966b3be5-4090-4316-a6ed-b2e4059190fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b83aaaab-a9e1-478d-901f-187693a8d189;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 426ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b83aaaab-a9e1-478d-901f-187693a8d189\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/16 22:47:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Exercise\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698106da-bce1-4000-8960-4b62d047713c",
   "metadata": {},
   "source": [
    "## Explore dataset FHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "964e856d-1c52-454a-880f-c4929b126f09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00254|2019-01-01 00:33:03|2019-01-01 01:37:24|         140|          52|   null|                B02356|\n",
      "|              B00254|2019-01-01 00:03:00|2019-01-01 00:34:25|         141|         237|   null|                B00254|\n",
      "|              B00254|2019-01-01 00:45:48|2019-01-01 01:26:01|         237|         236|   null|                B00254|\n",
      "|              B00254|2019-01-01 00:37:39|2019-01-01 01:44:59|         162|          85|   null|                B00254|\n",
      "|              B00254|2019-01-01 00:35:06|2019-01-01 01:30:21|         237|         246|   null|                B00254|\n",
      "|              B00254|2019-01-01 00:55:23|2019-01-01 01:48:27|         145|         224|   null|                B02882|\n",
      "|              B00254|2019-01-01 00:49:23|2019-01-01 01:38:38|         261|          14|   null|                B02994|\n",
      "|              B00254|2019-01-01 00:11:10|2019-01-01 00:44:31|         162|         233|   null|                B00254|\n",
      "|              B00254|2019-01-01 00:00:06|2019-01-01 00:32:21|          13|          87|   null|                B00254|\n",
      "|              B00254|2019-01-01 00:36:32|2019-01-01 01:35:54|         249|         236|   null|                B00254|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(path=\"fhv_tripdata_2019-01.csv\", sep=\",\", header=True)\n",
    "data.filter(data.PUlocationID.isNotNull()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758ff6b-18fe-470d-b655-221466eba004",
   "metadata": {},
   "source": [
    "## Explore dataset Green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50dbe6d3-20df-4f51-a835-9bc74ce1efe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2018-12-21 15:17:29|  2018-12-21 15:18:57|                 N|         1|         264|         264|              5|          .00|          3|  0.5|    0.5|         0|           0|     null|                  0.3|         4.3|           2|        1|                null|\n",
      "|       2| 2019-01-01 00:10:16|  2019-01-01 00:16:32|                 N|         1|          97|          49|              2|          .86|          6|  0.5|    0.5|         0|           0|     null|                  0.3|         7.3|           2|        1|                null|\n",
      "|       2| 2019-01-01 00:27:11|  2019-01-01 00:31:38|                 N|         1|          49|         189|              2|          .66|        4.5|  0.5|    0.5|         0|           0|     null|                  0.3|         5.8|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:46:20|  2019-01-01 01:04:54|                 N|         1|         189|          17|              2|         2.68|       13.5|  0.5|    0.5|      2.96|           0|     null|                  0.3|       19.71|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:19:06|  2019-01-01 00:39:43|                 N|         1|          82|         258|              1|         4.53|         18|  0.5|    0.5|         0|           0|     null|                  0.3|        19.3|           2|        1|                null|\n",
      "|       2| 2019-01-01 00:12:35|  2019-01-01 00:19:09|                 N|         1|          49|          17|              1|         1.05|        6.5|  0.5|    0.5|         0|           0|     null|                  0.3|         7.8|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:47:55|  2019-01-01 01:00:01|                 N|         1|         255|          33|              1|         3.77|       13.5|  0.5|    0.5|         0|           0|     null|                  0.3|        14.8|           1|        1|                null|\n",
      "|       1| 2019-01-01 00:12:47|  2019-01-01 00:30:50|                 N|         1|          76|         225|              1|         4.10|         16|  0.5|    0.5|         0|           0|     null|                  0.3|        17.3|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:16:23|  2019-01-01 00:39:46|                 N|         1|          25|          89|              1|         7.75|       25.5|  0.5|    0.5|         0|           0|     null|                  0.3|        26.8|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:58:02|  2019-01-01 01:19:02|                 N|         1|          85|          39|              1|         3.68|       15.5|  0.5|    0.5|         0|           0|     null|                  0.3|        16.8|           1|        1|                null|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataGreen = spark.read.csv(path=\"green_tripdata_2019-01.csv\", sep=\",\", header=True)\n",
    "dataGreen.filter(dataGreen.PULocationID.isNotNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ab19b08-6d5a-479b-b2f5-5d33ca7de2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', StringType(), True), StructField('lpep_pickup_datetime', StringType(), True), StructField('lpep_dropoff_datetime', StringType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('passenger_count', StringType(), True), StructField('trip_distance', StringType(), True), StructField('fare_amount', StringType(), True), StructField('extra', StringType(), True), StructField('mta_tax', StringType(), True), StructField('tip_amount', StringType(), True), StructField('tolls_amount', StringType(), True), StructField('ehail_fee', StringType(), True), StructField('improvement_surcharge', StringType(), True), StructField('total_amount', StringType(), True), StructField('payment_type', StringType(), True), StructField('trip_type', StringType(), True), StructField('congestion_surcharge', StringType(), True)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataGreen.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744ecf1-a67e-4b6c-819e-eee906d26389",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading from Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63ea4e3-631f-4bcb-b08a-40600ec7f564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_ride_from_kafka_message(df_raw, schema):\n",
    "    \"\"\" take a Spark Streaming df and parse value col based on <schema>, return streaming df cols in schema \"\"\"\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    # split attributes to nested array in one Column\n",
    "    col = F.split(df['value'], ', ')\n",
    "\n",
    "    # expand col to multiple top-level columns\n",
    "    for idx, field in enumerate(schema):\n",
    "        df = df.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n",
    "    return df.select([field.name for field in schema])\n",
    "\n",
    "def sink_memory(df, query_name, query_template):\n",
    "    write_query = df \\\n",
    "        .writeStream \\\n",
    "        .queryName(query_name) \\\n",
    "        .format('memory') \\\n",
    "        .start()\n",
    "    query_str = query_template.format(table_name=query_name)\n",
    "    query_results = spark.sql(query_str)\n",
    "    return write_query, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6fabe02-f6a7-4224-a167-4997865a6eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fhv_schema = T.StructType(\n",
    "    [T.StructField(\"dispatching_base_num\", T.StringType()),\n",
    "     T.StructField('pickup_datetime', T.TimestampType()),\n",
    "     T.StructField('dropOff_datetime', T.TimestampType()),\n",
    "     T.StructField(\"PUlocationID\", T.IntegerType()),\n",
    "     T.StructField(\"DOlocationID\", T.FloatType()),\n",
    "     T.StructField(\"Affiliated_base_number\", T.FloatType()),\n",
    "     ])\n",
    "\n",
    "green_schema = T.StructType(\n",
    "    [T.StructField('VendorID', T.StringType(), ), \n",
    "    T.StructField('lpep_pickup_datetime', T.TimestampType(), ), \n",
    "    T.StructField('lpep_dropoff_datetime', T.TimestampType(), ), \n",
    "    T.StructField('store_and_fwd_flag', T.StringType(), ), \n",
    "    T.StructField('RatecodeID', T.IntegerType(), ), \n",
    "    T.StructField('PULocationID',T.IntegerType(), ), \n",
    "    T.StructField('DOLocationID', T.IntegerType(), )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90d4376f-9c09-4612-a5cb-33a88924ad54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# default for startingOffsets is \"latest\"\n",
    "df_kafka_raw_fhv = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "    .option(\"subscribe\", \"fhv_csv\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()\n",
    "\n",
    "# default for startingOffsets is \"latest\"\n",
    "df_kafka_raw_green = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "    .option(\"subscribe\", \"green_csv\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0116a99e-4e6d-4ac6-864f-96e9bb2baacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rides_fhv = parse_ride_from_kafka_message(df_raw=df_kafka_raw_fhv, schema=fhv_schema)\n",
    "df_rides_green = parse_ride_from_kafka_message(df_raw=df_kafka_raw_green, schema=green_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b34169a9-732b-4123-8e12-1b6cb836968b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: float (nullable = true)\n",
      " |-- Affiliated_base_number: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rides_fhv.printSchema()\n",
    "df_rides_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12c1a9ff-960a-4666-9a60-0e46120b7dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/16 23:06:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-06fcef72-fbea-446e-8722-32827a81d5b5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/16 23:06:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/03/16 23:06:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1f427527-ebdd-4855-827b-6a15ca1dd03a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/16 23:06:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/03/16 23:06:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-driver-0-30, groupId=spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/16 23:06:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-driver-0-30, groupId=spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "23/03/16 23:06:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-executor-32, groupId=spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-executor] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/16 23:06:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-executor-32, groupId=spark-kafka-source-81769b31-a19c-45c3-a828-e43f1548cebd-542195457-executor] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    }
   ],
   "source": [
    "query_name = 'q_fhv3'\n",
    "query_template = 'select PUlocationID from {table_name}'\n",
    "write_query, df_popular_pulocation_fhv = sink_memory(df=df_rides_fhv, query_name=query_name, query_template=query_template)\n",
    "\n",
    "query_name = 'q_green3'\n",
    "query_template = 'select PULocationID from {table_name}'\n",
    "write_query, df_popular_pulocation_green = sink_memory(df=df_rides_green, query_name=query_name, query_template=query_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b66ea-5492-4d2b-95ac-dd4fd0547475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_popular_pulocation_fhv.show()\n",
    "df_popular_pulocation_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b893a3ad-1ad9-4e69-a8ff-2e097e7694c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_total = df_popular_pulocation_fhv.union(df_popular_pulocation_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f464ca-1802-4fca-8d71-2d6e01b33829",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Result\n",
    "Most popular PuLocationID is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd466404-1829-4b0c-99f3-925e3bdeb31c",
   "metadata": {
    "tags": []
   },
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_total.groupBy('PUlocationID') \\\n",
    "    .count() \\\n",
    "    .sort(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
